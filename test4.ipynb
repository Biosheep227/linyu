{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c102046e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import *\n",
    "from keras import callbacks\n",
    "from tensorflow.keras.layers import *\n",
    "from keras.layers import *\n",
    "from PIL import Image\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4cdfaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_activities(df_s):\n",
    "    activitiesSeq = []\n",
    "    ponentialIndex = df_s.label.ne(df_s.label.shift())\n",
    "    ii = np.where(ponentialIndex == True)[0]\n",
    "    for i_s, end in enumerate(ii):\n",
    "        if i_s > 0:\n",
    "            df_stmp = df_s[ii[i_s - 1]:end]\n",
    "            activitiesSeq.append(df_stmp)\n",
    "    return activitiesSeq\n",
    "\n",
    "def sequences_to_sentences(activity_sequences_s):\n",
    "    sentences_s = []\n",
    "    label_sentences_s = []\n",
    "    for i_s in range(len(activity_sequences_s)):\n",
    "        sentence = generate_sentence(activity_sequences_s[i_s])\n",
    "        sentences_s.append(sentence)\n",
    "        label_sentences_s.append(activity_sequences_s[i_s].label.values[0])\n",
    "    return sentences_s, label_sentences_s\n",
    "\n",
    "def generate_sentence(df2):\n",
    "    sentence = \"\"\n",
    "    sensors = df2.sensor.values\n",
    "#    values = df2.status.values\n",
    "    for i_s in range(len(sensors)):\n",
    "#        val = values[i_s]\n",
    "        if i_s == len(sensors) - 1:\n",
    "            sentence += \"{}\".format(sensors[i_s])\n",
    "        else:\n",
    "            sentence += \"{} \".format(sensors[i_s])\n",
    "    return sentence\n",
    "\n",
    "def sliding_window(sequence, win_size_s, step_s=1):\n",
    "    try:\n",
    "        iter(sequence)\n",
    "    except TypeError:\n",
    "        raise Exception(\"**ERROR** sequence must be iterable.\")\n",
    "    numOfChunks = int(((len(sequence) - win_size_s) / step_s) + 1)\n",
    "\n",
    "    if win_size_s > len(sequence):\n",
    "        yield sequence[0:len(sequence)]\n",
    "    else:\n",
    "        for i_s in range(0, numOfChunks * step_s, step_s):\n",
    "            yield sequence[i_s:i_s + win_size_s]\n",
    "\n",
    "# 定义一个函数来填充1和0.5之间的值\n",
    "def fill_between_values(col, df, col_name):\n",
    "    first_one_index = col[col == 1].index.min()\n",
    "    # 如果存在1的值，将其之前的所有值填充为0.5\n",
    "    if first_one_index is not None:\n",
    "        df.loc[:first_one_index - 1, col_name] = 0.5\n",
    "\n",
    "    start_index = None\n",
    "    previous_value = None\n",
    "    for idx, value in col.items():\n",
    "        if value == 1 or value == 0.5:\n",
    "            if start_index is not None:\n",
    "                if previous_value == 1:\n",
    "                    df.loc[start_index:idx-1, col_name] = 1\n",
    "                elif previous_value == 0.5:\n",
    "                    df.loc[start_index:idx-1, col_name] = 0.5\n",
    "                start_index = None\n",
    "            previous_value = value\n",
    "        elif start_index is None:\n",
    "            start_index = idx\n",
    "\n",
    "    return df[col_name]\n",
    "\n",
    "def count_directories(path):\n",
    "    dir_count = 0\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        # 在第一层停止，不遍历子目录\n",
    "        dir_count += len(dirs)\n",
    "        break  # 只处理顶层目录，不进入子目录\n",
    "    return dir_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f526e68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "         date      time sensor status\n",
      "0  2023/10/13  14:55:03      1    1.0\n",
      "1  2023/10/13  14:55:04    100    1.0\n",
      "2  2023/10/13  14:55:09      2    1.0\n",
      "3  2023/10/13  14:55:10      3    1.0\n",
      "4  2023/10/13  14:55:11     61    1.0\n",
      "5  2023/10/13  14:55:11      1    0.5\n",
      "6  2023/10/13  14:55:11    102    1.0\n",
      "7  2023/10/13  14:55:12    101    1.0\n",
      "8  2023/10/13  14:55:13     61    0.5\n",
      "9  2023/10/13  14:55:13      2    0.5\n",
      "         date      time  sensor  status  100  101  102  103  104  105  106\n",
      "0  2023/10/13  14:55:03       1     1.0  0.5  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "1  2023/10/13  14:55:04     100     1.0  1.0  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "2  2023/10/13  14:55:09       2     1.0  1.0  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "3  2023/10/13  14:55:10       3     1.0  1.0  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "4  2023/10/13  14:55:11      61     1.0  1.0  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "5  2023/10/13  14:55:11       1     0.5  1.0  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "6  2023/10/13  14:55:11     102     1.0  1.0  0.5  1.0  0.5  0.5  0.5  0.5\n",
      "7  2023/10/13  14:55:12     101     1.0  1.0  1.0  1.0  0.5  0.5  0.5  0.5\n",
      "8  2023/10/13  14:55:13      61     0.5  1.0  1.0  1.0  0.5  0.5  0.5  0.5\n",
      "9  2023/10/13  14:55:13       2     0.5  1.0  1.0  1.0  0.5  0.5  0.5  0.5\n",
      "          date      time  sensor  status  100  101  102  103  104  105  106\n",
      "0   2023/10/13  14:55:03       1     1.0  0.5  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "2   2023/10/13  14:55:09       2     1.0  1.0  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "3   2023/10/13  14:55:10       3     1.0  1.0  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "4   2023/10/13  14:55:11      61     1.0  1.0  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "5   2023/10/13  14:55:11       1     0.5  1.0  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "8   2023/10/13  14:55:13      61     0.5  1.0  1.0  1.0  0.5  0.5  0.5  0.5\n",
      "9   2023/10/13  14:55:13       2     0.5  1.0  1.0  1.0  0.5  0.5  0.5  0.5\n",
      "10  2023/10/13  14:55:13       3     0.5  1.0  1.0  1.0  0.5  0.5  0.5  0.5\n",
      "11  2023/10/13  14:55:19      11     1.0  1.0  1.0  1.0  0.5  0.5  0.5  0.5\n",
      "12  2023/10/13  14:55:23      11     0.5  1.0  1.0  1.0  0.5  0.5  0.5  0.5\n",
      "          date      time  sensor  status                                label\n",
      "0   2023/10/13  14:55:03       1     1.0  (0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5)\n",
      "2   2023/10/13  14:55:09       2     1.0  (1.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5)\n",
      "3   2023/10/13  14:55:10       3     1.0  (1.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5)\n",
      "4   2023/10/13  14:55:11      61     1.0  (1.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5)\n",
      "5   2023/10/13  14:55:11       1     0.5  (1.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5)\n",
      "8   2023/10/13  14:55:13      61     0.5  (1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5)\n",
      "9   2023/10/13  14:55:13       2     0.5  (1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5)\n",
      "10  2023/10/13  14:55:13       3     0.5  (1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5)\n",
      "11  2023/10/13  14:55:19      11     1.0  (1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5)\n",
      "12  2023/10/13  14:55:23      11     0.5  (1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5)\n",
      "          date      time  sensor  status                                label\n",
      "0   2023/10/13  14:55:03     1.0     1.0  (0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5)\n",
      "2   2023/10/13  14:55:09     2.0     1.0  (1.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5)\n",
      "3   2023/10/13  14:55:10     3.0     1.0  (1.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5)\n",
      "4   2023/10/13  14:55:11    61.0     1.0  (1.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5)\n",
      "5   2023/10/13  14:55:11     0.9     0.5  (1.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5)\n",
      "8   2023/10/13  14:55:13    60.9     0.5  (1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5)\n",
      "9   2023/10/13  14:55:13     1.9     0.5  (1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5)\n",
      "10  2023/10/13  14:55:13     2.9     0.5  (1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5)\n",
      "11  2023/10/13  14:55:19    11.0     1.0  (1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5)\n",
      "12  2023/10/13  14:55:23    10.9     0.5  (1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('1013_clean.csv', header=None, names=[\"date\", \"time\", \"sensor\", \"status\"])\n",
    "df = df.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "print(type(df))\n",
    "print(df[:10])\n",
    "\n",
    "# 将 'sensor' 列转换为整数\n",
    "df = df[df['sensor'] != 'sensor']\n",
    "df['sensor'] = pd.to_numeric(df['sensor'])\n",
    "df['status'] = df['status'].astype(float)\n",
    "\n",
    "# 定义特殊的sensor值\n",
    "special_sensors = [100,101,102,103,104,105,106]\n",
    "\n",
    "# 对每个特殊的sensor进行处理\n",
    "for sensor in special_sensors:\n",
    "    # 新增一列，并为符合条件的行填充对应的值\n",
    "    df[sensor] = (df[\"sensor\"] == sensor).astype(int) * df[\"status\"]\n",
    "\n",
    "for col_name in df.columns[4:]:\n",
    "    df[col_name] = fill_between_values(df[col_name], df, col_name)\n",
    "print(df[:10])\n",
    "\n",
    "\n",
    "for sensor in special_sensors:\n",
    "        # 删除包含特殊sensor值的行\n",
    "    df = df[df[\"sensor\"] != sensor]\n",
    "\n",
    "print(df[:10])\n",
    "\n",
    "df['label'] = df[special_sensors].apply(tuple, axis=1)\n",
    "df.drop(columns=special_sensors, inplace=True)\n",
    "\n",
    "print(df[:10])\n",
    "\n",
    "# 获取 'status' 和 'sensor' 列\n",
    "# 删除包含错误值的行\n",
    "df = df[df['sensor'] != 'sensor']\n",
    "df['sensor'] = pd.to_numeric(df['sensor'])\n",
    "sensor_column = df['sensor'].astype(float)\n",
    "status_column = df['status'].astype(float)\n",
    "\n",
    "# 将 'status' 列中所有1变成0, 所有0.5变成-0.1\n",
    "status_column[status_column == 1] = 0\n",
    "status_column[status_column == 0.5] = -0.1\n",
    "\n",
    "# 将处理后的 'status' 列和 'sensor' 列相加，并将结果赋值给 'sensor' 列\n",
    "df['sensor'] = status_column + sensor_column\n",
    "\n",
    "# 输出结果\n",
    "print(df[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "470374f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: segment dataset in sequence of activity\n",
      "STEP 3: segment dataset in sequence of activity\n",
      "[           date      time  sensor  status                                label\n",
      "876  2023/10/13  15:35:48     0.9     0.5  (1.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5),            date      time  sensor  status                                label\n",
      "878  2023/10/13  15:36:04     1.0     1.0  (0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5),            date      time  sensor  status                                label\n",
      "880  2023/10/13  15:36:10     2.0     1.0  (1.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5)\n",
      "881  2023/10/13  15:36:11     3.0     1.0  (1.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5)\n",
      "882  2023/10/13  15:36:12     0.9     0.5  (1.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5)\n",
      "883  2023/10/13  15:36:12    61.0     1.0  (1.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5),            date      time  sensor  status                                label\n",
      "885  2023/10/13  15:36:14     1.9     0.5  (1.0, 0.5, 1.0, 0.5, 0.5, 0.5, 0.5)\n",
      "886  2023/10/13  15:36:14    60.9     0.5  (1.0, 0.5, 1.0, 0.5, 0.5, 0.5, 0.5)\n",
      "887  2023/10/13  15:36:15    11.0     1.0  (1.0, 0.5, 1.0, 0.5, 0.5, 0.5, 0.5)\n",
      "888  2023/10/13  15:36:16     2.9     0.5  (1.0, 0.5, 1.0, 0.5, 0.5, 0.5, 0.5)]\n",
      "STEP 4: transform sequences of activity in sentences\n",
      "['1.0', '2.0 3.0 61.0 0.9', '60.9 1.9 2.9 11.0 10.9', '11.0', '3.0 61.0 2.0 60.9', '10.9 27.0 22.0 2.9 21.0 1.9 24.0 23.0 25.0 26.9 20.9 24.9', '25.0', '21.9 22.0 24.9 21.9 25.0', '23.9 24.9 22.9 24.0 25.0 22.0 23.0 21.0 27.0', '24.9 21.9 61.0 22.9 20.9 3.0 62.0 26.9 2.9 63.0 64.0 60.9 61.9 62.0']\n",
      "[(0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5), (1.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5), (1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5), (0.5, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5), (1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5), (1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.5), (1.0, 0.5, 1.0, 1.0, 0.5, 0.5, 0.5), (1.0, 0.5, 0.5, 1.0, 0.5, 0.5, 0.5), (0.5, 0.5, 0.5, 1.0, 0.5, 0.5, 0.5), (0.5, 0.5, 0.5, 1.0, 1.0, 0.5, 0.5)]\n",
      "STEP 5: sentences indexization\n",
      "[[1.0], [2.0, 3.0, 61.0, 0.9], [60.9, 1.9, 2.9, 11.0, 10.9], [11.0], [3.0, 61.0, 2.0, 60.9]]\n",
      "STEP 6: split indexed sentences in sliding windows\n",
      "[[1.0], [2.0, 3.0, 61.0, 0.9], [60.9, 1.9, 2.9, 11.0, 10.9], [11.0], [3.0, 61.0, 2.0, 60.9], [10.9, 27.0, 22.0, 2.9, 21.0, 1.9, 24.0, 23.0], [27.0, 22.0, 2.9, 21.0, 1.9, 24.0, 23.0, 25.0], [22.0, 2.9, 21.0, 1.9, 24.0, 23.0, 25.0, 26.9], [2.9, 21.0, 1.9, 24.0, 23.0, 25.0, 26.9, 20.9], [21.0, 1.9, 24.0, 23.0, 25.0, 26.9, 20.9, 24.9]]\n",
      "[(0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5), (1.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5), (1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5), (0.5, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5), (1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5), (1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.5), (1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.5), (1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.5), (1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.5), (1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.5)]\n",
      "STEP 7: pad sliding windows\n",
      "[[ 1.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 2.   3.  61.   0.9  0.   0.   0.   0. ]\n",
      " [60.9  1.9  2.9 11.  10.9  0.   0.   0. ]\n",
      " [11.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 3.  61.   2.  60.9  0.   0.   0.   0. ]\n",
      " [10.9 27.  22.   2.9 21.   1.9 24.  23. ]\n",
      " [27.  22.   2.9 21.   1.9 24.  23.  25. ]\n",
      " [22.   2.9 21.   1.9 24.  23.  25.  26.9]\n",
      " [ 2.9 21.   1.9 24.  23.  25.  26.9 20.9]\n",
      " [21.   1.9 24.  23.  25.  26.9 20.9 24.9]]\n",
      "[[0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0]\n",
      " [1 1 1 0 0 0 0]\n",
      " [0 1 1 0 0 0 0]\n",
      " [1 1 1 0 0 0 0]\n",
      " [1 1 1 1 0 0 0]\n",
      " [1 1 1 1 0 0 0]\n",
      " [1 1 1 1 0 0 0]\n",
      " [1 1 1 1 0 0 0]\n",
      " [1 1 1 1 0 0 0]]\n",
      "<class 'numpy.ndarray'>\n",
      "328\n",
      "STEP 8: save sliding windows and labels\n"
     ]
    }
   ],
   "source": [
    "#  Segment dataset in sequence of activity ##\n",
    "print(\"STEP 1: segment dataset in sequence of activity\")\n",
    "\n",
    "#  Segment dataset in sequence of activity ##\n",
    "print(\"STEP 3: segment dataset in sequence of activity\")\n",
    "activity_sequences = segment_activities(df)\n",
    "print(activity_sequences[121:125])\n",
    "\n",
    "#  Transform sequences of activity in sentences ##\n",
    "print(\"STEP 4: transform sequences of activity in sentences\")\n",
    "sentences, label_sentences = sequences_to_sentences(activity_sequences)\n",
    "print(sentences[:10])\n",
    "print(label_sentences[:10])\n",
    "\n",
    "#  Indexization ##\n",
    "print(\"STEP 5: sentences indexization\")\n",
    "# 转换文本为整数序列\n",
    "sequences = [list(map(float, s.split())) for s in sentences]\n",
    "#sequences = [np.array(seq) for seq in sentences]\n",
    "\n",
    "print(sequences[:5])\n",
    "\n",
    "#  Split in sliding windows ##\n",
    "print(\"STEP 6: split indexed sentences in sliding windows\")\n",
    "X_windowed = []\n",
    "Y_windowed = []\n",
    "X_windowed_sen = []\n",
    "Y_windowed_sen = []\n",
    "win_size = 8\n",
    "step = 1\n",
    "for i, s in enumerate(sequences):\n",
    "    chunks = sliding_window(s, win_size, step)\n",
    "    for chunk in chunks:\n",
    "        X_windowed.append(chunk)\n",
    "        Y_windowed.append(label_sentences[i])\n",
    "print(X_windowed[0:10])\n",
    "print(Y_windowed[0:10])\n",
    "\n",
    "#  Pad windows ##\n",
    "print(\"STEP 7: pad sliding windows\")\n",
    "#padded_windows = pad_sequences(X_windowed, padding ='post', value=0.0)\n",
    "\n",
    "# 获取最大序列长度\n",
    "max_length = max(len(seq) for seq in X_windowed)\n",
    "\n",
    "# 使用 np.pad 函数填充每个序列\n",
    "padded_windows = [np.pad(seq, (0, max_length - len(seq)), mode='constant', constant_values=0.0) for seq in X_windowed]\n",
    "\n",
    "# 将结果转换为 NumPy 数组\n",
    "padded_windows = np.array(padded_windows)\n",
    "\n",
    "Y_windowed = np.array(Y_windowed)\n",
    "Y_windowed = Y_windowed.astype(int)\n",
    "print(padded_windows[:10])\n",
    "print(Y_windowed[:10])\n",
    "print(type(Y_windowed))\n",
    "print(len(Y_windowed))\n",
    "#  Save files ##\n",
    "print(\"STEP 8: save sliding windows and labels\")\n",
    "# np.save(\"{}_{}_padded_x_step1_state_linyu.npy\".format(r'/Users/zehaokou/Desktop/Technion/AI/plot/Linyu_dataset/', win_size), padded_windows)\n",
    "# np.save(\"label_sentences.npy\".format(r'/Users/zehaokou/Desktop/Technion/AI/plot/Linyu_dataset', win_size), Y_windowed)\n",
    "# pickle_file_path = '/Users/zehaokou/Desktop/Technion/AI/plot/Linyu_dataset/indexed_sentences.pkl'\n",
    "# with open(pickle_file_path, 'wb') as pickle_file:\n",
    "#     pickle.dump(indexed_sentences, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23c6d192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化一个空的图像数据列表和标签列表\n",
    "image_data = []\n",
    "labels = []\n",
    "\n",
    "for i in range(len(padded_windows)):\n",
    "    \n",
    "    label = Y_windowed[i]\n",
    "#    label = label.astype(str)\n",
    "    label = np.array2string(label, precision=2, separator=',', suppress_small=True)\n",
    "    labels.append(label)\n",
    "    \n",
    "label_encoder = LabelEncoder()\n",
    "int_labels = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90f3a968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "['[0,0,0,0,0,0,0]', '[1,0,0,0,0,0,0]', '[1,1,1,0,0,0,0]', '[0,1,1,0,0,0,0]', '[1,1,1,0,0,0,0]', '[1,1,1,1,0,0,0]', '[1,1,1,1,0,0,0]', '[1,1,1,1,0,0,0]', '[1,1,1,1,0,0,0]', '[1,1,1,1,0,0,0]']\n",
      "[]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'list'>\n",
      "{0: '[0,0,0,0,0,0,0]', 1: '[0,0,0,0,0,0,1]', 2: '[0,0,0,0,0,1,0]', 3: '[0,0,0,0,1,0,0]', 4: '[0,0,0,0,1,0,1]', 5: '[0,0,0,0,1,1,0]', 6: '[0,0,0,1,0,0,0]', 7: '[0,0,0,1,1,0,0]', 8: '[0,1,1,0,0,0,0]', 9: '[0,1,1,1,0,0,0]', 10: '[1,0,0,0,0,0,0]', 11: '[1,0,0,1,0,0,0]', 12: '[1,0,1,0,0,0,0]', 13: '[1,0,1,1,0,0,0]', 14: '[1,1,1,0,0,0,0]', 15: '[1,1,1,1,0,0,0]'}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(padded_windows)):\n",
    "    \n",
    "    label = int_labels[i]\n",
    "    #label = label.astype(str)\n",
    "    label = np.array2string(label, precision=2, separator=',', suppress_small=True)\n",
    "    \n",
    "#    label_dir = f\"/Users/zehaokou/Desktop/Technion/AI/plot/Linyu_dataset/1013/plot_train_state_7/{label}\"\n",
    "    label_dir = f\"images/plot_train_state_0/{label}\"\n",
    "    \n",
    "    zero = np.zeros((100,100))\n",
    "    for j in range(6):\n",
    "        index = 99 - padded_windows[i][j]\n",
    "        if index != 0:\n",
    "            if index - index.astype(int) != 0:\n",
    "                index = index.astype(int)\n",
    "                zero[index][2*j:(2*j+1)] = 0.5\n",
    "            elif index - index.astype(int) == 0:\n",
    "                index = index.astype(int)\n",
    "                zero[index][2*j:(2*j+1)] = 1\n",
    "            else:\n",
    "                zero[index][2*j:(2*j+1)] = 0\n",
    "\n",
    "#############填充状态s    \n",
    "    for row in range(100):\n",
    "        # 找到当前纵坐标下所有值不为0的点的横坐标\n",
    "        cols_with_value = [col for col in range(100) if zero[row][col] > 0]\n",
    "\n",
    "        if len(cols_with_value) > 0:\n",
    "            # 如果第一个值是0.5，从y=0向这一点连线\n",
    "            if zero[row][cols_with_value[0]] == 0.5:\n",
    "                zero[row][0:cols_with_value[0]] = 1\n",
    "\n",
    "            # 对相邻的on和off之间进行填充\n",
    "            for k in range(0, len(cols_with_value) - 1):\n",
    "                # If the current point is 'on' and the next point is 'off', fill between them\n",
    "                if zero[row][cols_with_value[k]] == 1 and zero[row][cols_with_value[k + 1]] == 0.5:\n",
    "                    zero[row][cols_with_value[k]:cols_with_value[k + 1]] = 1\n",
    "\n",
    "            # 如果最后一个值为1，从这点向y=not_nan_count*2 - 1连线\n",
    "            if zero[row][cols_with_value[-1]] == 1:\n",
    "                zero[row][cols_with_value[-1]:2*win_size-1] = 1\n",
    "\n",
    "    for row in range(100):\n",
    "# 找到当前纵坐标下所有值为1的点的横坐标\n",
    "        cols_with_value = [col for col in range(100) if zero[row][col] == 1]\n",
    "\n",
    "        # 将除了值为1的点外的其他点设置为0\n",
    "        zero[row, :] = 0\n",
    "        for col in cols_with_value:\n",
    "            zero[row, col] = 1\n",
    "        ###########\n",
    "\n",
    "    # 将zero数组的值乘以255\n",
    "    data = zero * 255\n",
    "    # 将data数组的数据类型转换为'uint8'\n",
    "    data = data.astype('uint8')\n",
    "    # 确保保存图像的目录存在，如果不存在就创建\n",
    "    if not os.path.exists(label_dir):\n",
    "        os.makedirs(label_dir)\n",
    "    \n",
    "    # 生成图片文件的完整路径，这里使用一个计数器以防止覆盖同一标签下的不同图片\n",
    "#    img_path = os.path.join(label_dir, f\"{label}_{i}.png\")\n",
    "    img_path = os.path.join(label_dir, f\"{label}_{i}.png\")\n",
    "    \n",
    "    # 将图像数据保存为图片\n",
    "    img = Image.fromarray(data)\n",
    "    img.save(img_path)\n",
    "    \n",
    "    # 重置zero数组为224x224的零数组，为下一次迭代做准备\n",
    "    zero = np.zeros((100, 100))\n",
    "\n",
    "\n",
    "print('done')\n",
    "print(labels[:10])\n",
    "print(image_data[:10])\n",
    "#labels = labels.cpu().numpy()\n",
    "labels = np.array(labels)\n",
    "print(type(labels))\n",
    "print(type(image_data))\n",
    "\n",
    "#labels = np.array(labels)\n",
    "label_encoder = LabelEncoder()\n",
    "int_labels = label_encoder.fit_transform(labels)\n",
    "mapping = dict(zip(range(len(label_encoder.classes_)), label_encoder.classes_))\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "377207b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of directories: 16\n"
     ]
    }
   ],
   "source": [
    "# 使用方式\n",
    "# 使用 str.rsplit 分隔字符串，并保留前面的部分\n",
    "path, _ = label_dir.rsplit('/', 1)\n",
    "\n",
    "number_of_directories = count_directories(path)\n",
    "print(f'Number of directories: {number_of_directories}')\n",
    "\n",
    "# 指定数据集路径和类别数量\n",
    "\n",
    "dataset_path = path\n",
    "num_classes = number_of_directories\n",
    "\n",
    "# 获取每个类别的路径和图像文件名列表\n",
    "class_paths = []\n",
    "for i in range(num_classes):\n",
    "    \n",
    "    class_path = os.path.join(dataset_path, \"{}\".format(i))\n",
    "    class_files = os.listdir(class_path)\n",
    "#    print(class_files[:10])\n",
    "    class_paths.append((class_path, class_files))\n",
    "\n",
    "# 打乱每个类别的图像列表\n",
    "for class_path, class_files in class_paths:\n",
    "    random.shuffle(class_files)\n",
    "\n",
    "# 将数据集分成训练集和测试集\n",
    "train_file = open(dataset_path + \"/train.txt\", \"w\")\n",
    "test_file = open(dataset_path + \"/test.txt\", \"w\")\n",
    "for i, (class_path, class_files) in enumerate(class_paths):\n",
    "    num_train = int(len(class_files) * 0.7)\n",
    "    for j, filename in enumerate(class_files):\n",
    "        if j < num_train:\n",
    "            train_file.write(\"{} {}\\n\".format(os.path.join(class_path, filename), i))\n",
    "        else:\n",
    "            test_file.write(\"{} {}\\n\".format(os.path.join(class_path, filename), i))\n",
    "train_file.close()\n",
    "test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d198ad2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is not available. Using CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查是否有可用的GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    print(\"MPS is not available. Using CPU.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "## resnet\n",
    "#net = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "net = torchvision.models.resnet34(weights='IMAGENET1K_V1')\n",
    "net.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "net.to(device)  # 将模型移动到CUDA设备上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "003baf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载成功！\n",
      "FOLD 1/10\n",
      "------------------------------ \n",
      " epoch 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/liumingyi/Desktop/work/python/linyu/test4.ipynb 单元格 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liumingyi/Desktop/work/python/linyu/test4.ipynb#X11sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m inputs, labels \u001b[39m=\u001b[39m data\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liumingyi/Desktop/work/python/linyu/test4.ipynb#X11sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m inputs, labels \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/liumingyi/Desktop/work/python/linyu/test4.ipynb#X11sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m outputs \u001b[39m=\u001b[39m net(inputs)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liumingyi/Desktop/work/python/linyu/test4.ipynb#X11sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/liumingyi/Desktop/work/python/linyu/test4.ipynb#X11sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m val_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/zehao/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/zehao/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/zehao/lib/python3.11/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/zehao/lib/python3.11/site-packages/torchvision/models/resnet.py:276\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    274\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(x)\n\u001b[1;32m    275\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x)\n\u001b[0;32m--> 276\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer4(x)\n\u001b[1;32m    278\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[1;32m    279\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/zehao/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/zehao/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/zehao/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/zehao/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/zehao/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/zehao/lib/python3.11/site-packages/torchvision/models/resnet.py:92\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m     90\u001b[0m     identity \u001b[39m=\u001b[39m x\n\u001b[0;32m---> 92\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[1;32m     93\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(out)\n\u001b[1;32m     94\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/anaconda3/envs/zehao/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/zehao/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/zehao/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/zehao/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 自定义图片图片读取方式\n",
    "def MyLoader(path):\n",
    "    return Image.open(path).convert('L')\n",
    "\n",
    "# def MyLoader(path):\n",
    "#     return np.load(path, allow_pickle=True)\n",
    "    \n",
    "    \n",
    "class MyDataset (Dataset):\n",
    "    # 构造函数设置默认参数\n",
    "    def __init__(self, txt, transform=None, target_transform=None, loader=MyLoader):\n",
    "        with open(txt, 'r') as fh:\n",
    "            imgs = []\n",
    "            for line in fh:\n",
    "                line = line.strip('\\n')  # 移除字符串首尾的换行符\n",
    "                line = line.rstrip()  # 删除末尾空\n",
    "                words = line.split( )  # 以空格为分隔符 将字符串分成\n",
    "                imgs.append((words[0], int(words[1]))) # imgs中包含有图像路径和标签\n",
    "        self.imgs = imgs\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.loader = loader\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fn, label = self.imgs[index]\n",
    "        #调用定义的loader方法\n",
    "        img = self.loader(fn)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "\n",
    "\n",
    "root = path + '/'\n",
    "train_data = MyDataset(txt=root + 'train.txt', transform=transforms.ToTensor())\n",
    "test_data = MyDataset(txt=root + 'test.txt', transform=transforms.ToTensor())\n",
    "\n",
    "test_num = len(test_data)\n",
    "\n",
    "#train_data 和test_data包含多有的训练与测试数据，调用DataLoader批量加载\n",
    "trainloader = DataLoader(dataset=train_data, batch_size=32, shuffle=True)\n",
    "testloader = DataLoader(dataset=test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "print('加载成功！')\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.optim as optim\n",
    "\n",
    "# CrossEntropyLoss就是我们需要的损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Parameters\n",
    "patience = 50  # for early stopping\n",
    "n_folds = 10   # for k-fold cross-validation\n",
    "num_epochs = 200\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "# kfold = KFold(n_splits=n_folds, shuffle=True, random_state=5)\n",
    "kfold = KFold(n_splits=n_folds, shuffle=True)\n",
    "early_stop_counter = 0\n",
    "best_loss = float('inf')\n",
    "best_model_wts = None\n",
    "\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(trainloader.dataset)):\n",
    "    print(f\"FOLD {fold + 1}/{n_folds}\")\n",
    "    \n",
    "    # Define your network and optimizer here, they should be re-initialized for each fold\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
    "    \n",
    "    \n",
    "    trainloader_fold = torch.utils.data.DataLoader(trainloader.dataset, batch_size=32, sampler=train_subsampler)\n",
    "    valloader_fold = torch.utils.data.DataLoader(trainloader.dataset, batch_size=32, sampler=val_subsampler)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('-'*30, '\\n','epoch', epoch)\n",
    "        net.train()\n",
    "        loss100 = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, data in enumerate(trainloader_fold):\n",
    "            # Training code remains largely the same...\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device) # 注意需要复制到GPU\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss100 += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Validation\n",
    "        net.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(valloader_fold):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(valloader_fold)\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Check for early stopping\n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            best_model_wts = net.state_dict().copy()  # 更新最佳模型的权重\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        if early_stop_counter >= patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "# 保存最佳模型的权重到文件\n",
    "model_save_path = path +'/' +'best_model.pth'\n",
    "torch.save(best_model_wts, model_save_path)\n",
    "# Finally, you could report the average performance across all folds.\n",
    "#plt.plot(train_accuracies)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "274d508d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 73.333333 %\n",
      "Loss : 3.031840 %\n",
      "Balanced Accuracy : 47.410714 %\n",
      "[[ 2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  1  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  1  0  0  0  0  1  0  0  0  0  0]\n",
      " [ 0  0  0 21  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  2  0  1  2  0  0  0  0  0  1  0  0  0  0  0]\n",
      " [ 0  1  1  0  2  1  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 28  2  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  1 11  0  0  0  0  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  2  0  2  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  1  1  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  1  2  0  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  5]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "predicted_list = []\n",
    "labels_list = []\n",
    "# 构造测试的dataloader\n",
    "dataiter = iter(testloader)\n",
    "# 预测正确的数量和总数量\n",
    "correct = 0\n",
    "total = 0\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # 预测\n",
    "        outputs = net(images)\n",
    "        # 输出概率分布，最大概率的一项作为预测分类\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        # 将预测结果和真实结果添加到列表中\n",
    "        predicted_list.extend(predicted.cpu().numpy())\n",
    "        labels_list.extend(labels.cpu().numpy())\n",
    "        # 计算损失\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "\n",
    "confusion_mat = confusion_matrix(labels_list, predicted_list)\n",
    "print('Accuracy : %f %%' % (\n",
    "    100 * correct / test_num))\n",
    "print('Loss : %f %%' % (\n",
    "    loss.item()))\n",
    "balanced_accuracy = balanced_accuracy_score(labels_list, predicted_list, adjusted=False)\n",
    "print('Balanced Accuracy : %f %%' %(100 * balanced_accuracy))\n",
    "print(confusion_mat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
