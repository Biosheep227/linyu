{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47d64af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import *\n",
    "from keras import callbacks\n",
    "from tensorflow.keras.layers import *\n",
    "from keras.layers import *\n",
    "import random\n",
    "from PIL import Image\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# begin_time = \"2023-09-09 18:55:00\"\n",
    "# end_time = \"2023-09-09 19:02:06\"\n",
    "# ap = {\n",
    "#     \"apicode\": \"5fb9d287\",\n",
    "#     \"begin_time\": begin_time,\n",
    "#     \"end_time\": end_time\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9b4b7d",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "285fa46d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date      time sensor  status   客厅   餐厅   入口\n",
      "0   2023-09-09  18:55:12      6     1.0  0.0  0.0  0.0\n",
      "1   2023-09-09  18:55:12      4     1.0  0.0  0.0  0.0\n",
      "2   2023-09-09  18:55:12      5     1.0  0.0  0.0  0.0\n",
      "3   2023-09-09  18:55:13      3     1.0  0.0  0.0  0.0\n",
      "4   2023-09-09  18:55:13      2     1.0  0.0  0.0  0.0\n",
      "5   2023-09-09  18:55:14     客厅     1.0  1.0  0.0  0.0\n",
      "6   2023-09-09  18:55:17     44     1.0  0.0  0.0  0.0\n",
      "7   2023-09-09  18:55:28      6     0.5  0.0  0.0  0.0\n",
      "8   2023-09-09  18:55:28      5     0.5  0.0  0.0  0.0\n",
      "9   2023-09-09  18:55:29      6     1.0  0.0  0.0  0.0\n",
      "10  2023-09-09  18:55:29      2     0.5  0.0  0.0  0.0\n",
      "11  2023-09-09  18:55:32     44     0.5  0.0  0.0  0.0\n",
      "12  2023-09-09  18:55:34      4     0.5  0.0  0.0  0.0\n",
      "13  2023-09-09  18:55:37      2     1.0  0.0  0.0  0.0\n",
      "14  2023-09-09  18:55:37      4     1.0  0.0  0.0  0.0\n",
      "15  2023-09-09  18:55:38     44     1.0  0.0  0.0  0.0\n",
      "16  2023-09-09  18:55:54     44     0.5  0.0  0.0  0.0\n",
      "17  2023-09-09  18:56:02     44     1.0  0.0  0.0  0.0\n",
      "18  2023-09-09  18:56:05      5     1.0  0.0  0.0  0.0\n",
      "19  2023-09-09  18:56:06      1     1.0  0.0  0.0  0.0\n",
      "{(0.5, 0.5, 0.5): '0', (1, 0.5, 0.5): '1', (0.5, 1, 0.5): '2', (0.5, 0.5, 1): '3', (1, 1, 0.5): '4', (1, 0.5, 1): '5', (0.5, 1, 1): '6', (1, 1, 1): '7'}\n",
      "STEP 3: segment dataset in sequence of activity\n",
      "[         date      time sensor  status label\n",
      "0  2023-09-09  18:55:12      6     1.0     0\n",
      "1  2023-09-09  18:55:12      4     1.0     0\n",
      "2  2023-09-09  18:55:12      5     1.0     0\n",
      "3  2023-09-09  18:55:13      3     1.0     0\n",
      "4  2023-09-09  18:55:13      2     1.0     0,           date      time sensor  status label\n",
      "6   2023-09-09  18:55:17     44     1.0     1\n",
      "7   2023-09-09  18:55:28      6     0.5     1\n",
      "8   2023-09-09  18:55:28      5     0.5     1\n",
      "9   2023-09-09  18:55:29      6     1.0     1\n",
      "10  2023-09-09  18:55:29      2     0.5     1\n",
      "11  2023-09-09  18:55:32     44     0.5     1\n",
      "12  2023-09-09  18:55:34      4     0.5     1\n",
      "13  2023-09-09  18:55:37      2     1.0     1\n",
      "14  2023-09-09  18:55:37      4     1.0     1\n",
      "15  2023-09-09  18:55:38     44     1.0     1\n",
      "16  2023-09-09  18:55:54     44     0.5     1\n",
      "17  2023-09-09  18:56:02     44     1.0     1\n",
      "18  2023-09-09  18:56:05      5     1.0     1\n",
      "19  2023-09-09  18:56:06      1     1.0     1\n",
      "20  2023-09-09  18:56:12      4     0.5     1\n",
      "21  2023-09-09  18:56:15      4     1.0     1\n",
      "22  2023-09-09  18:56:17     44     0.5     1\n",
      "23  2023-09-09  18:56:26     43     1.0     1\n",
      "24  2023-09-09  18:56:26     42     1.0     1,           date      time sensor  status label\n",
      "26  2023-09-09  18:56:26     23     1.0     4\n",
      "27  2023-09-09  18:56:28     22     1.0     4\n",
      "28  2023-09-09  18:56:28      3     0.5     4\n",
      "29  2023-09-09  18:56:29     21     1.0     4\n",
      "30  2023-09-09  18:56:30     24     1.0     4\n",
      "31  2023-09-09  18:56:31      4     0.5     4\n",
      "32  2023-09-09  18:56:36      6     0.5     4\n",
      "33  2023-09-09  18:56:37      5     0.5     4\n",
      "34  2023-09-09  18:56:39      1     0.5     4\n",
      "35  2023-09-09  18:56:40      2     0.5     4\n",
      "36  2023-09-09  18:56:41     42     0.5     4\n",
      "37  2023-09-09  18:56:41     43     0.5     4\n",
      "38  2023-09-09  18:56:42      4     1.0     4,           date      time sensor  status label\n",
      "40  2023-09-09  18:56:47     24     0.5     2\n",
      "41  2023-09-09  18:56:55     21     0.5     2\n",
      "42  2023-09-09  18:56:57     23     0.5     2\n",
      "43  2023-09-09  18:56:58      4     0.5     2\n",
      "44  2023-09-09  18:57:08     21     1.0     2\n",
      "45  2023-09-09  18:57:16      4     1.0     2\n",
      "46  2023-09-09  18:57:25     21     0.5     2\n",
      "47  2023-09-09  18:57:32      4     0.5     2\n",
      "48  2023-09-09  18:57:40     22     0.5     2\n",
      "49  2023-09-09  18:57:41     22     1.0     2\n",
      "50  2023-09-09  18:57:50     23     1.0     2\n",
      "51  2023-09-09  18:57:51     21     1.0     2\n",
      "52  2023-09-09  18:57:52     24     1.0     2\n",
      "53  2023-09-09  18:57:52      4     1.0     2\n",
      "54  2023-09-09  18:58:06     23     0.5     2\n",
      "55  2023-09-09  18:58:23     23     1.0     2\n",
      "56  2023-09-09  18:58:29     21     0.5     2\n",
      "57  2023-09-09  18:58:29      4     0.5     2\n",
      "58  2023-09-09  18:58:31     21     1.0     2\n",
      "59  2023-09-09  18:58:40      4     1.0     2\n",
      "60  2023-09-09  18:58:46     23     0.5     2\n",
      "61  2023-09-09  18:58:55     24     0.5     2\n",
      "62  2023-09-09  18:58:55     21     0.5     2\n",
      "63  2023-09-09  18:58:56     22     0.5     2\n",
      "64  2023-09-09  18:58:56      4     0.5     2\n",
      "65  2023-09-09  18:59:20     22     1.0     2\n",
      "66  2023-09-09  18:59:20     21     1.0     2\n",
      "67  2023-09-09  18:59:29     23     1.0     2\n",
      "68  2023-09-09  18:59:29     24     1.0     2\n",
      "69  2023-09-09  18:59:30      4     1.0     2\n",
      "70  2023-09-09  18:59:45     23     0.5     2\n",
      "71  2023-09-09  18:59:47     23     1.0     2\n",
      "72  2023-09-09  18:59:52     43     1.0     2\n",
      "73  2023-09-09  18:59:52     42     1.0     2\n",
      "74  2023-09-09  18:59:52      3     1.0     2\n",
      "75  2023-09-09  18:59:53      2     1.0     2\n",
      "76  2023-09-09  18:59:53     44     1.0     2,           date      time sensor  status label\n",
      "78  2023-09-09  18:59:55     41     1.0     6\n",
      "79  2023-09-09  18:59:59     21     0.5     6\n",
      "80  2023-09-09  19:00:02     22     0.5     6\n",
      "81  2023-09-09  19:00:04     23     0.5     6\n",
      "82  2023-09-09  19:00:06      4     0.5     6\n",
      "83  2023-09-09  19:00:06     24     0.5     6\n",
      "84  2023-09-09  19:00:07     43     0.5     6\n",
      "85  2023-09-09  19:00:08     44     0.5     6\n",
      "86  2023-09-09  19:00:09      2     0.5     6\n",
      "87  2023-09-09  19:00:21     42     0.5     6\n",
      "88  2023-09-09  19:00:23     42     1.0     6\n",
      "89  2023-09-09  19:00:35     41     0.5     6\n",
      "90  2023-09-09  19:00:39     42     0.5     6\n",
      "91  2023-09-09  19:00:42      3     0.5     6,            date      time sensor  status label\n",
      "93   2023-09-09  19:00:53     41     1.0     3\n",
      "94   2023-09-09  19:00:55      3     1.0     3\n",
      "95   2023-09-09  19:00:56     42     1.0     3\n",
      "96   2023-09-09  19:01:20     42     0.5     3\n",
      "97   2023-09-09  19:01:28     41     0.5     3\n",
      "98   2023-09-09  19:01:28      3     0.5     3\n",
      "99   2023-09-09  19:01:32     41     1.0     3\n",
      "100  2023-09-09  19:01:46      3     1.0     3\n",
      "101  2023-09-09  19:01:47     42     1.0     3\n",
      "102  2023-09-09  19:01:49     43     1.0     3\n",
      "103  2023-09-09  19:01:49      2     1.0     3\n",
      "104  2023-09-09  19:01:50     23     1.0     3\n",
      "105  2023-09-09  19:01:50     44     1.0     3,            date      time sensor  status label\n",
      "107  2023-09-09  19:01:52      4     1.0     5\n",
      "108  2023-09-09  19:01:52      1     1.0     5\n",
      "109  2023-09-09  19:01:53      6     1.0     5\n",
      "110  2023-09-09  19:02:00     41     0.5     5\n",
      "111  2023-09-09  19:02:02      3     0.5     5\n",
      "112  2023-09-09  19:02:03     42     0.5     5\n",
      "113  2023-09-09  19:02:04     43     0.5     5\n",
      "114  2023-09-09  19:02:05      2     0.5     5\n",
      "115  2023-09-09  19:02:06     23     0.5     5\n",
      "116  2023-09-09  19:02:08      4     0.5     5\n",
      "117  2023-09-09  19:02:08      1     0.5     5\n",
      "118  2023-09-09  19:02:09      6     0.5     5\n",
      "119  2023-09-09  19:02:10      3     1.0     5\n",
      "120  2023-09-09  19:02:11     44     0.5     5\n",
      "121  2023-09-09  19:02:25      4     1.0     5\n",
      "122  2023-09-09  19:02:28      6     1.0     5\n",
      "123  2023-09-09  19:02:28      2     1.0     5\n",
      "124  2023-09-09  19:02:30      5     1.0     5\n",
      "125  2023-09-09  19:02:41      6     0.5     5\n",
      "126  2023-09-09  19:02:42      6     1.0     5,            date      time sensor  status label\n",
      "128  2023-09-09  19:03:02      5     0.5     1\n",
      "129  2023-09-09  19:03:18      2     0.5     1\n",
      "130  2023-09-09  19:03:21      2     1.0     1\n",
      "131  2023-09-09  19:03:24      5     1.0     1\n",
      "132  2023-09-09  19:03:34      1     1.0     1\n",
      "133  2023-09-09  19:03:40      5     0.5     1\n",
      "134  2023-09-09  19:03:42      3     0.5     1\n",
      "135  2023-09-09  19:03:45      3     1.0     1\n",
      "136  2023-09-09  19:03:47      2     0.5     1\n",
      "137  2023-09-09  19:03:50      1     0.5     1\n",
      "138  2023-09-09  19:03:52      6     0.5     1\n",
      "139  2023-09-09  19:03:57      6     1.0     1\n",
      "140  2023-09-09  19:04:28      6     0.5     1\n",
      "141  2023-09-09  19:04:31      6     1.0     1\n",
      "142  2023-09-09  19:04:39      3     0.5     1\n",
      "143  2023-09-09  19:04:42      3     1.0     1\n",
      "144  2023-09-09  19:04:51      5     1.0     1\n",
      "145  2023-09-09  19:04:53      2     1.0     1\n",
      "146  2023-09-09  19:04:54      1     1.0     1\n",
      "147  2023-09-09  19:04:55     43     1.0     1\n",
      "148  2023-09-09  19:04:56     42     1.0     1\n",
      "149  2023-09-09  19:04:56     23     1.0     1\n",
      "150  2023-09-09  19:04:57     44     1.0     1,            date      time sensor  status label\n",
      "152  2023-09-09  19:04:59     41     1.0     5\n",
      "153  2023-09-09  19:05:05      6     0.5     5\n",
      "154  2023-09-09  19:05:07      5     0.5     5\n",
      "155  2023-09-09  19:05:08      2     0.5     5\n",
      "156  2023-09-09  19:05:10      1     0.5     5\n",
      "157  2023-09-09  19:05:12     44     0.5     5\n",
      "158  2023-09-09  19:05:12      4     0.5     5\n",
      "159  2023-09-09  19:05:12     23     0.5     5\n",
      "160  2023-09-09  19:05:25     43     0.5     5\n",
      "161  2023-09-09  19:05:26      3     0.5     5,            date      time sensor  status label\n",
      "163  2023-09-09  19:05:44      3     1.0     3\n",
      "164  2023-09-09  19:06:04      2     1.0     3\n",
      "165  2023-09-09  19:06:05     43     1.0     3\n",
      "166  2023-09-09  19:06:23      2     0.5     3\n",
      "167  2023-09-09  19:06:38     41     0.5     3\n",
      "168  2023-09-09  19:06:39     43     0.5     3\n",
      "169  2023-09-09  19:06:43     41     1.0     3\n",
      "170  2023-09-09  19:06:59      3     0.5     3\n",
      "171  2023-09-09  19:07:04     41     0.5     3\n",
      "172  2023-09-09  19:07:04     42     0.5     3\n",
      "173  2023-09-09  19:07:08     41     1.0     3\n",
      "174  2023-09-09  19:07:13      3     1.0     3\n",
      "175  2023-09-09  19:07:14     42     1.0     3\n",
      "176  2023-09-09  19:07:18     43     1.0     3\n",
      "177  2023-09-09  19:07:18     23     1.0     3\n",
      "178  2023-09-09  19:07:18     44     1.0     3\n",
      "179  2023-09-09  19:07:20      4     1.0     3]\n",
      "STEP 4: transform sequences of activity in sentences\n",
      "['6 4 5 3 2', '44 6 5 6 2 44 4 2 4 44 44 44 5 1 4 4 44 43 42', '23 22 3 21 24 4 6 5 1 2 42 43 4', '24 21 23 4 21 4 21 4 22 22 23 21 24 4 23 23 21 4 21 4 23 24 21 22 4 22 21 23 24 4 23 23 43 42 3 2 44', '41 21 22 23 4 24 43 44 2 42 42 41 42 3', '41 3 42 42 41 3 41 3 42 43 2 23 44', '4 1 6 41 3 42 43 2 23 4 1 6 3 44 4 6 2 5 6 6', '5 2 2 5 1 5 3 3 2 1 6 6 6 6 3 3 5 2 1 43 42 23 44', '41 6 5 2 1 44 4 23 43 3', '3 2 43 2 41 43 41 3 41 42 41 3 42 43 23 44 4']\n",
      "['0', '1', '4', '2', '6', '3', '5', '1', '5', '3']\n",
      "STEP 5: sentences indexization\n",
      "[[6, 4, 5, 3, 2], [44, 6, 5, 6, 2, 44, 4, 2, 4, 44, 44, 44, 5, 1, 4, 4, 44, 43, 42], [23, 22, 3, 21, 24, 4, 6, 5, 1, 2, 42, 43, 4], [24, 21, 23, 4, 21, 4, 21, 4, 22, 22, 23, 21, 24, 4, 23, 23, 21, 4, 21, 4, 23, 24, 21, 22, 4, 22, 21, 23, 24, 4, 23, 23, 43, 42, 3, 2, 44], [41, 21, 22, 23, 4, 24, 43, 44, 2, 42, 42, 41, 42, 3]]\n",
      "STEP 6: split indexed sentences in sliding windows\n",
      "[[6, 4, 5, 3, 2], [44, 6, 5, 6, 2, 44, 4, 2], [6, 5, 6, 2, 44, 4, 2, 4], [5, 6, 2, 44, 4, 2, 4, 44], [6, 2, 44, 4, 2, 4, 44, 44], [2, 44, 4, 2, 4, 44, 44, 44], [44, 4, 2, 4, 44, 44, 44, 5], [4, 2, 4, 44, 44, 44, 5, 1], [2, 4, 44, 44, 44, 5, 1, 4], [4, 44, 44, 44, 5, 1, 4, 4]]\n",
      "['0', '1', '1', '1', '1', '1', '1', '1', '1', '1']\n",
      "STEP 7: pad sliding windows\n",
      "[[ 6  4  5  3  2  0  0  0]\n",
      " [44  6  5  6  2 44  4  2]\n",
      " [ 6  5  6  2 44  4  2  4]\n",
      " [ 5  6  2 44  4  2  4 44]\n",
      " [ 6  2 44  4  2  4 44 44]\n",
      " [ 2 44  4  2  4 44 44 44]\n",
      " [44  4  2  4 44 44 44  5]\n",
      " [ 4  2  4 44 44 44  5  1]\n",
      " [ 2  4 44 44 44  5  1  4]\n",
      " [ 4 44 44 44  5  1  4  4]]\n",
      "['0' '1' '1' '1' '1' '1' '1' '1' '1' '1']\n",
      "STEP 8: save sliding windows and labels\n"
     ]
    }
   ],
   "source": [
    "def segment_activities(df_s):\n",
    "    activitiesSeq = []\n",
    "    ponentialIndex = df_s.label.ne(df_s.label.shift())\n",
    "    ii = np.where(ponentialIndex == True)[0]\n",
    "    for i_s, end in enumerate(ii):\n",
    "        if i_s > 0:\n",
    "            df_stmp = df_s[ii[i_s - 1]:end]\n",
    "            activitiesSeq.append(df_stmp)\n",
    "    return activitiesSeq\n",
    "\n",
    "def sequences_to_sentences(activity_sequences_s):\n",
    "    sentences_s = []\n",
    "    label_sentences_s = []\n",
    "    for i_s in range(len(activity_sequences_s)):\n",
    "        sentence = generate_sentence(activity_sequences_s[i_s])\n",
    "        sentences_s.append(sentence)\n",
    "        label_sentences_s.append(activity_sequences_s[i_s].label.values[0])\n",
    "    return sentences_s, label_sentences_s\n",
    "\n",
    "def generate_sentence(df2):\n",
    "    sentence = \"\"\n",
    "    sensors = df2.sensor.values\n",
    "#    values = df2.status.values\n",
    "    for i_s in range(len(sensors)):\n",
    "#        val = values[i_s]\n",
    "        if i_s == len(sensors) - 1:\n",
    "            sentence += \"{}\".format(sensors[i_s])\n",
    "        else:\n",
    "            sentence += \"{} \".format(sensors[i_s])\n",
    "    return sentence\n",
    "\n",
    "def sliding_window(sequence, win_size_s, step_s=1):\n",
    "    try:\n",
    "        iter(sequence)\n",
    "    except TypeError:\n",
    "        raise Exception(\"**ERROR** sequence must be iterable.\")\n",
    "    numOfChunks = int(((len(sequence) - win_size_s) / step_s) + 1)\n",
    "\n",
    "    if win_size_s > len(sequence):\n",
    "        yield sequence[0:len(sequence)]\n",
    "    else:\n",
    "        for i_s in range(0, numOfChunks * step_s, step_s):\n",
    "            yield sequence[i_s:i_s + win_size_s]\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('linyu.csv', header=None, names=[\"date\", \"time\", \"sensor\", \"status\"])\n",
    "df = df.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "# 定义特殊的sensor值\n",
    "special_sensors = ['客厅', '餐厅', '入口']\n",
    "\n",
    "# 对每个特殊的sensor进行处理\n",
    "for sensor in special_sensors:\n",
    "    # 新增一列，并为符合条件的行填充对应的值\n",
    "    df[sensor] = (df[\"sensor\"] == sensor) * df[\"status\"]\n",
    "\n",
    "print(df[:20])\n",
    "# 定义一个函数来填充1和0.5之间的值\n",
    "def fill_between_values(col, df, col_name):\n",
    "    first_one_index = col[col == 1].index.min()\n",
    "    # 如果存在1的值，将其之前的所有值填充为0.5\n",
    "    if first_one_index is not None:\n",
    "        df.loc[:first_one_index - 1, col_name] = 0.5\n",
    "\n",
    "    start_index = None\n",
    "    previous_value = None\n",
    "    for idx, value in col.items():\n",
    "        if value == 1 or value == 0.5:\n",
    "            if start_index is not None:\n",
    "                if previous_value == 1:\n",
    "                    df.loc[start_index:idx-1, col_name] = 1\n",
    "                elif previous_value == 0.5:\n",
    "                    df.loc[start_index:idx-1, col_name] = 0.5\n",
    "                start_index = None\n",
    "            previous_value = value\n",
    "        elif start_index is None:\n",
    "            start_index = idx\n",
    "\n",
    "    return df[col_name]\n",
    "\n",
    "for col_name in df.columns[-3:]:\n",
    "    df[col_name] = fill_between_values(df[col_name], df, col_name)\n",
    "    \n",
    "combinations = {\n",
    "    (0.5, 0.5, 0.5): '0',\n",
    "    (1, 0.5, 0.5): '1',\n",
    "    (0.5, 1, 0.5): '2',\n",
    "    (0.5, 0.5, 1): '3',\n",
    "    (1, 1, 0.5): '4',\n",
    "    (1, 0.5, 1): '5',\n",
    "    (0.5, 1, 1): '6',\n",
    "    (1, 1, 1): '7',\n",
    "}\n",
    "\n",
    "# def generate_combinations(values, num_positions):\n",
    "#     # 使用itertools.product生成所有的组合\n",
    "#     all_combinations = list(product(values, repeat=num_positions))\n",
    "    \n",
    "#     # 转换为\"{0.5,0.5,0.5}\"的格式，并作为标签\n",
    "#     formatted_combinations = {f\"{{{','.join(map(str, combo))}}}\": f\"{{{','.join(map(str, combo))}}}\" for combo in all_combinations}\n",
    "#     return formatted_combinations\n",
    "\n",
    "# values = [1, 0.5]\n",
    "# num_positions = 3\n",
    "# combinations = generate_combinations(values, num_positions)\n",
    "\n",
    "print(combinations)\n",
    "\n",
    "# 应用转换字典\n",
    "df['label'] = df.apply(lambda row: combinations.get((row['客厅'], row['餐厅'], row['入口']), 'none'), axis=1)\n",
    "\n",
    "df = df[~df['sensor'].isin(['客厅', '餐厅', '入口'])]\n",
    "df = df.drop(columns=['客厅', '餐厅', '入口'])\n",
    "#print(df[:50])\n",
    "\n",
    "#  Segment dataset in sequence of activity ##\n",
    "print(\"STEP 3: segment dataset in sequence of activity\")\n",
    "activity_sequences = segment_activities(df)\n",
    "print(activity_sequences[:10])\n",
    "\n",
    "#  Transform sequences of activity in sentences ##\n",
    "print(\"STEP 4: transform sequences of activity in sentences\")\n",
    "sentences, label_sentences = sequences_to_sentences(activity_sequences)\n",
    "print(sentences[:10])\n",
    "print(label_sentences[:10])\n",
    "\n",
    "#  Indexization ##\n",
    "print(\"STEP 5: sentences indexization\")\n",
    "# 转换文本为整数序列\n",
    "sequences = [list(map(int, s.split())) for s in sentences]\n",
    "#sequences = [np.array(seq) for seq in sentences]\n",
    "\n",
    "print(sequences[:5])\n",
    "\n",
    "#  Split in sliding windows ##\n",
    "print(\"STEP 6: split indexed sentences in sliding windows\")\n",
    "X_windowed = []\n",
    "Y_windowed = []\n",
    "X_windowed_sen = []\n",
    "Y_windowed_sen = []\n",
    "win_size = 8\n",
    "step = 1\n",
    "for i, s in enumerate(sequences):\n",
    "    chunks = sliding_window(s, win_size, step)\n",
    "    for chunk in chunks:\n",
    "        X_windowed.append(chunk)\n",
    "        Y_windowed.append(label_sentences[i])\n",
    "print(X_windowed[0:10])\n",
    "print(Y_windowed[0:10])\n",
    "\n",
    "#  Pad windows ##\n",
    "print(\"STEP 7: pad sliding windows\")\n",
    "padded_windows = pad_sequences(X_windowed, padding ='post')\n",
    "Y_windowed = np.array(Y_windowed)\n",
    "print(padded_windows[0:10])\n",
    "print(Y_windowed[0:10])\n",
    "\n",
    "#  Save files ##\n",
    "print(\"STEP 8: save sliding windows and labels\")\n",
    "# np.save(\"{}_{}_padded_x_step1_state_linyu.npy\".format(r'/Users/zehaokou/Desktop/Technion/AI/plot/Linyu_dataset/', win_size), padded_windows)\n",
    "# np.save(\"label_sentences.npy\".format(r'/Users/zehaokou/Desktop/Technion/AI/plot/Linyu_dataset', win_size), Y_windowed)\n",
    "# pickle_file_path = '/Users/zehaokou/Desktop/Technion/AI/plot/Linyu_dataset/indexed_sentences.pkl'\n",
    "# with open(pickle_file_path, 'wb') as pickle_file:\n",
    "#     pickle.dump(indexed_sentences, pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9e7bb9",
   "metadata": {},
   "source": [
    "# 准备npy数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f4058e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = padded_windows\n",
    "\n",
    "y = Y_windowed\n",
    "y = y.astype(str)\n",
    "\n",
    "activities = {'0': [], '1': [], '2': [], '3': [], '4': [], '5': [], '6': [], '7': []}\n",
    "\n",
    "for i, value in enumerate(y):\n",
    "    if value in activities:\n",
    "        activities[value].append(i)\n",
    "\n",
    "activity0 = activities['0']\n",
    "activity1 = activities['1']\n",
    "activity2 = activities['2']\n",
    "activity3 = activities['3']\n",
    "activity4 = activities['4']\n",
    "activity5 = activities['5']\n",
    "activity6 = activities['6']\n",
    "activity7 = activities['7']\n",
    "\n",
    "        \n",
    "zero = np.zeros((50,50))\n",
    "\n",
    "for i in range(1000):\n",
    "    for j in range(8):\n",
    "        # 如果是nan，则跳过此次循环\n",
    "        if np.isnan(x[activity1[i]][j]):\n",
    "            continue\n",
    "            \n",
    "        index = 49 - x[activity1[i]][j]\n",
    "        if index != 0:\n",
    "            if index - index.astype(int) != 0:\n",
    "                index = index.astype(int)\n",
    "                zero[index][2*j:(2*j+1)] = 0.5\n",
    "            elif index - index.astype(int) == 0:\n",
    "                index = index.astype(int)\n",
    "                zero[index][2*j:(2*j+1)] = 1\n",
    "            else:\n",
    "                zero[index][2*j:(2*j+1)] = 0\n",
    "                \n",
    "    #计算这个窗口中不为nan的数的个数\n",
    "    not_nan_count = np.sum(~np.isnan(x[activity1[i]]))\n",
    "   #print(not_nan_count)\n",
    "  \n",
    "    for row in range(50):\n",
    "        # 找到当前纵坐标下所有值不为0的点的横坐标\n",
    "        cols_with_value = [col for col in range(50) if zero[row][col] > 0]\n",
    "        \n",
    "        if len(cols_with_value) > 0:\n",
    "            # 如果第一个值是0.5，从y=0向这一点连线\n",
    "            if zero[row][cols_with_value[0]] == 0.5:\n",
    "                zero[row][0:cols_with_value[0]] = 1\n",
    "\n",
    "            # 对相邻的on和off之间进行填充\n",
    "            for k in range(0, len(cols_with_value) - 1):\n",
    "                # If the current point is 'on' and the next point is 'off', fill between them\n",
    "                if zero[row][cols_with_value[k]] == 1 and zero[row][cols_with_value[k + 1]] == 0.5:\n",
    "                    zero[row][cols_with_value[k]:cols_with_value[k + 1]] = 1\n",
    "\n",
    "            # 如果最后一个值为1，从这点向y=not_nan_count*2 - 1连线\n",
    "            if zero[row][cols_with_value[-1]] == 1:\n",
    "                zero[row][cols_with_value[-1]:2*not_nan_count-1] = 1\n",
    "        \n",
    "    for row in range(50):\n",
    "# 找到当前纵坐标下所有值为1的点的横坐标\n",
    "        cols_with_value = [col for col in range(50) if zero[row][col] == 1]\n",
    "\n",
    "        # 将除了值为1的点外的其他点设置为0\n",
    "        zero[row, :] = 0\n",
    "        for col in cols_with_value:\n",
    "            zero[row, col] = 1\n",
    "            \n",
    "    # 将zero数组的值乘以255\n",
    "    data = zero * 255\n",
    "    # 将data数组的数据类型转换为'uint8'\n",
    "    data = data.astype('uint8')\n",
    "\n",
    "    np.save(\"/images/1/%d.npy\" % (i),data)\n",
    "\n",
    "    # 重置zero数组为224x224的零数组，为下一次迭代做准备\n",
    "    zero = np.zeros((50, 50))\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b21bef",
   "metadata": {},
   "source": [
    "# 生成txt文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4df6526",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/images\"\n",
    "num_classes = 8\n",
    "\n",
    "# 获取每个类别的路径和图像文件名列表\n",
    "class_paths = []\n",
    "for i in range(num_classes):\n",
    "    class_path = os.path.join(dataset_path, \"{}\".format(i))\n",
    "    class_files = [f for f in os.listdir(class_path) if \"DS\" not in f]\n",
    "    class_paths.append((class_path, class_files))\n",
    "\n",
    "# 打乱每个类别的图像列表\n",
    "for class_path, class_files in class_paths:\n",
    "    random.shuffle(class_files)\n",
    "\n",
    "# 将数据集分成训练集和测试集\n",
    "train_file = open(\"train.txt\", \"w\")\n",
    "test_file = open(\"test.txt\", \"w\")\n",
    "for i, (class_path, class_files) in enumerate(class_paths):\n",
    "    num_train = int(len(class_files) * 0.8)\n",
    "    for j, filename in enumerate(class_files):\n",
    "        if j < num_train:\n",
    "            train_file.write(\"{} {}\\n\".format(os.path.join(class_path, filename), i))\n",
    "        else:\n",
    "            test_file.write(\"{} {}\\n\".format(os.path.join(class_path, filename), i))\n",
    "train_file.close()\n",
    "test_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
